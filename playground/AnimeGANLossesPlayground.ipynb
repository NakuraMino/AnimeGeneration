{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnimeGANLossesPlayground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8645d9d1af34d62acfb7d737b1f97d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fab663b2648d4c62ac4e09354c4bc500",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6b590c1a4f1b435393cb6c392cd7adce",
              "IPY_MODEL_daa9e558a63f46d09d017e7b59dfec8e"
            ]
          }
        },
        "fab663b2648d4c62ac4e09354c4bc500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b590c1a4f1b435393cb6c392cd7adce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7774d5cd668e4e15b57f483742481db3",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_857be806c64c437b85a7a47f20f9f5a6"
          }
        },
        "daa9e558a63f46d09d017e7b59dfec8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_42bf72f9563d4eeab47ae59d31593d47",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:33&lt;00:00, 17.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d7a9b7a76a74cf39f4d15370f2a6631"
          }
        },
        "7774d5cd668e4e15b57f483742481db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "857be806c64c437b85a7a47f20f9f5a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42bf72f9563d4eeab47ae59d31593d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d7a9b7a76a74cf39f4d15370f2a6631": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4PM80Ex2pRm"
      },
      "source": [
        "# imports\n",
        "import torch \n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpRg-mDP2HRL"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWG6DbGq1L4D"
      },
      "source": [
        "From AnimeGAN paper: \"The loss function... used in AnimeGAN can be simply expressed as follows:  $L(G, D) = \\omega_{adv}L_{adv}(G,D) + \\omega_{con}L_{con}(G,D) + \\omega_{gra}L_{gra}(G, D) + \\omega_{col}L_{col}(G, D)$\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzCagkCX2KD7"
      },
      "source": [
        "### Adversarial Loss $L_{adv}(G,D)$\n",
        "\n",
        "\"In order to enable AnimeGAN to generate the higher quality images and\n",
        "make the training of the entire network more stable, the least squares loss function in LSGAN is employed as the adversarial loss $L_{adv}(G, D)$\"\n",
        "\n",
        "In other words..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECK4S0Mt2EGK"
      },
      "source": [
        "adversarial_loss = nn.MSELoss()\n",
        "\n",
        "# use: adversarial_loss(disciminator(generator(input)), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR4GiFcTOIGv",
        "outputId": "b21c59fb-a7fa-497f-8cf3-f57b3c134611"
      },
      "source": [
        "labels = torch.ones((4,1,1,1))\n",
        "noise_outputs = torch.randn(4,1,64,64)\n",
        "loss = adversarial_loss(noise_outputs, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([4, 1, 1, 1])) that is different to the input size (torch.Size([4, 1, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSL_Tu7V3F-T"
      },
      "source": [
        "### Content Loss $L_{con}(G,D)$\n",
        "\n",
        "\"$L_{con}(G,D)$ is the content loss which helps to make the generated image retain the content of the input photo\"\n",
        "\n",
        "\"For the content loss $L_{con}(G,D)$... the pretrained VGG19 is used as the perceptual network to extract high-level semantic features of the images. $L_{con}(G,D)$... can be expressed as:\n",
        "\n",
        "$L_{con}(G,D) = E_{p_{i}\\sim S_{data}(x)}[||VGG_l(p_i) - VGG_l(G(p_i))||_1]$\n",
        "\n",
        "where VGG_l(x) refers to the feature map of the lth layer in VGG... In our method, the lth layer is conv4-4 in VGG\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTePFC3i2wZS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "f8645d9d1af34d62acfb7d737b1f97d5",
            "fab663b2648d4c62ac4e09354c4bc500",
            "6b590c1a4f1b435393cb6c392cd7adce",
            "daa9e558a63f46d09d017e7b59dfec8e",
            "7774d5cd668e4e15b57f483742481db3",
            "857be806c64c437b85a7a47f20f9f5a6",
            "42bf72f9563d4eeab47ae59d31593d47",
            "0d7a9b7a76a74cf39f4d15370f2a6631"
          ]
        },
        "outputId": "edce8185-cb93-492c-ccc0-17f89eb8b7ec"
      },
      "source": [
        "import torchvision.models as models \n",
        "\n",
        "noise_inputs = torch.randn((4,3,256,256))\n",
        "\n",
        "vgg19 = models.vgg19(pretrained=True).eval() # eval means no backprop\n",
        "# out = vgg19(noise_inputs)\n",
        "# print(out.shape) # [4, 1000]\n",
        "# print(vgg19.features) # list of all weights in vgg19\n",
        "\n",
        "# We want to take up to the 25th feature according to:\n",
        "# https://www.researchgate.net/figure/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means_fig2_325137356\n",
        "VGG = nn.Sequential(*list(vgg19.features._modules.values())[:26]).eval()\n",
        "out = VGG(noise_inputs) # [4 x 512 x 32 x 32] feature map \n",
        "# print(out.shape)\n",
        "# print(VGG)\n",
        "\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html\n",
        "content_loss = nn.L1Loss()\n",
        "\n",
        "# use: takes in real photo p and generated photo g(p)\n",
        "#      content_loss(VGG(p), VGG(g(p)))\n",
        "\n",
        "# I'm a little concerned about how the backprop step works, especially since\n",
        "# we pass in our inputs through this network, which doesn't need backprop stuff, \n",
        "# but I suppose we'll manage."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8645d9d1af34d62acfb7d737b1f97d5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDDeEN9jOaua"
      },
      "source": [
        "class ContentLoss(nn.Module):\n",
        "  def __init__(self, VGG):\n",
        "    super(ContentLoss, self).__init__()\n",
        "    self.VGG = VGG\n",
        "    self.L1Loss = nn.L1Loss()\n",
        "  \n",
        "  def forward(self, generated, photo):\n",
        "    generated = self.VGG(generated)\n",
        "    photo = self.VGG(photo)\n",
        "    return self.L1Loss(generated, photo)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm5d4C6R3wBe"
      },
      "source": [
        "### Grayscale Style loss $L_{gra}(G,D)$\n",
        "\n",
        "\"$L_{gra}(G,D)$ is the grayscale style loss which makes the generated images have the clear anime style on the texture and lines\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZYyZAem4Cg5"
      },
      "source": [
        "from dataloader import *\n",
        "\n",
        "class GrayscaleStyleLoss(nn.Module):\n",
        "  \"\"\" grayscale style loss makes the generated images have\n",
        "      the clear anime style on the texture and lines\"\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, VGG):\n",
        "    super(GrayscaleStyleLoss, self).__init__()\n",
        "    self.VGG = VGG\n",
        "    self.L1Loss = nn.L1Loss()\n",
        "\n",
        "  @staticmethod\n",
        "  def gram_matrix(A):\n",
        "    \"\"\" @param A: image [N x C x H x W]\n",
        "        gram = A_unrolled @ A_unrolled.T\n",
        "        @returns: gram matrix of A, of shape [N, C, C]\n",
        "    \"\"\"\n",
        "    N,C,H,W = A.shape\n",
        "    A_unrolled = A.reshape((N,C,H*W))\n",
        "    A_unrolled_transpose = torch.transpose(A_unrolled, 1, 2)\n",
        "    gram = torch.bmm(A_unrolled, A_unrolled_transpose)\n",
        "    return gram\n",
        "\n",
        "  def forward(self, generated, anime_gray):\n",
        "    \"\"\" @param generated: images generated from generator, G(photo),\n",
        "                          of shape [N x C x H x W]\n",
        "        @param anime_gray: grayscale anime images, of shape\n",
        "                           [N x C x H x W]\n",
        "    \"\"\"\n",
        "    gram_generated = GrayscaleStyleLoss.gram_matrix(self.VGG(generated))\n",
        "    gram_anime_gray = GrayscaleStyleLoss.gram_matrix(self.VGG(anime_gray))\n",
        "    return self.L1Loss(gram_generated, gram_anime_gray) / generated.numel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf5x94kzK_GA"
      },
      "source": [
        "\r\n",
        "ANIME_PATH = '/content/drive/MyDrive/dataset/Shinkai/style/'\r\n",
        "# SMOOTH_PATH = '/content/drive/MyDrive/dataset/Shinkai/smooth/'\r\n",
        "PHOTOS_PATH = '/content/drive/MyDrive/dataset/train_photo/'\r\n",
        "\r\n",
        "# photo_dataloader = getPhotoDataloader(PHOTOS_PATH)\r\n",
        "anime_dataloader = getAnimeDataloader(ANIME_PATH, grayscale=True)\r\n",
        "aniter = iter(anime_dataloader)\r\n",
        "phiter = iter(photo_dataloader)\r\n",
        "\r\n",
        "\r\n",
        "def unstandardizeImage(images):\r\n",
        "    _mean=[0.485, 0.456, 0.406]; _std=[0.229, 0.224, 0.225]\r\n",
        "    output = torch.zeros(images.shape).cuda()\r\n",
        "    for i in range(3):\r\n",
        "      output[:,i,:,:] += images[:,i,:,:] * _std[i]\r\n",
        "      output[:,i,:,:] += images[:,i,:,:] + _mean[i]\r\n",
        "    output *= 255.0\r\n",
        "    return output\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "  photo_batch1 = next(phiter).cuda()\r\n",
        "  photo_batch2 = next(phiter).cuda()\r\n",
        "\r\n",
        "  anime_batch1 = next(aniter).cuda()\r\n",
        "  anime_batch2 = next(aniter).cuda()\r\n",
        "\r\n",
        "  gsl = GrayscaleStyleLoss(VGG.cuda())\r\n",
        "  diff = gsl(anime_batch1, photo_batch1)\r\n",
        "  print(diff.item())\r\n",
        "  same = gsl(anime_batch1, anime_batch2)\r\n",
        "  print(same.item())\r\n",
        "  same = gsl(photo_batch1, photo_batch2)\r\n",
        "  print(same.item())\r\n",
        "  print()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# out = gsl(noise_inputs, noise_inputs)\r\n",
        "# print(out) should be zero because they're the same inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlzR-ifO4DV-"
      },
      "source": [
        "### Color Reconstruction Loss $L_{col}(G,D)$\n",
        "\n",
        "\"$L_{col}(G,D)$ is used as the color reconstruction loss to make the generated images have the color of the original photos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhEtRA9D4T1C",
        "outputId": "5c4e571c-7da8-4b9e-c537-a805246b8654"
      },
      "source": [
        "class ColorReconLoss(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(ColorReconLoss, self).__init__()\n",
        "    self.L1Loss = nn.L1Loss()\n",
        "    self.HuberLoss = nn.SmoothL1Loss()\n",
        "  \n",
        "  @staticmethod\n",
        "  def rgb_to_ycbcr(input):\n",
        "    \"\"\" @param input: [N x 3 x H x W]\n",
        "        returns: YUV formatted version of the images \n",
        "        code is repurposed from here: \n",
        "        https://discuss.pytorch.org/t/how-to-change-a-batch-rgb-images-to-ycbcr-images-during-training/3799/2\n",
        "        formula is from: https://en.wikipedia.org/wiki/YCbCr\n",
        "    \"\"\"\n",
        "    output = torch.zeros(input.shape)\n",
        "    output[:, 0, :, :] = input[:, 0, :, :] * 65.481 + input[:, 1, :, :] * 128.553 + input[:, 2, :, :] * 24.966 + 16.\n",
        "    output[:, 1, :, :] = input[:, 0, :, :] * -37.797 + input[:, 1, :, :] * 74.203 + input[:, 2, :, :] * 112. + 128.\n",
        "    output[:, 2, :, :] = input[:, 0, :, :] * 112.0 + input[:, 1, :, :] * 93.786 + input[:, 2, :, :] * 18.214 + 128.\n",
        "    return output\n",
        "  \n",
        "  def forward(self, generated, real_photos):\n",
        "    \"\"\" @param generated: batch of generated anime images of RGB format,\n",
        "                          of shape [N x 3 x H x W]\n",
        "        @param real_photos: batch of real-life photos used to generate generated \n",
        "                            images, of shape [N x 3 x H x W]\n",
        "    \"\"\"\n",
        "    generated_yuv = ColorReconLoss.rgb_to_ycbcr(generated)\n",
        "    real_photos_yuv = ColorReconLoss.rgb_to_ycbcr(real_photos)\n",
        "    y_loss = self.L1Loss(generated_yuv[:,0,:,:], real_photos_yuv[:,0,:,:])\n",
        "    u_loss = self.HuberLoss(generated_yuv[:,1,:,:], real_photos_yuv[:,1,:,:])\n",
        "    v_loss = self.HuberLoss(generated_yuv[:,2,:,:], real_photos_yuv[:,2,:,:])\n",
        "    return y_loss + u_loss + v_loss\n",
        "\n",
        "crl = ColorReconLoss()\n",
        "out = crl(noise_inputs, noise_inputs)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awa-wYx6BTDc"
      },
      "source": [
        "# I think this basically is a Binary Cross Entropy Loss with Real Anime \n",
        "# images as 1, smoothed anime images as 0, and generated images as 0. \n",
        "# Basically, in our dataset, we just have to include all three types \n",
        "# (smooth=0, original=1, real=0) of images in the dataset."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ragnL7lvA0Kt"
      },
      "source": [
        "## Edge Promoting Adversarial Loss \n",
        "\n",
        "$L_{adv}(G,D) = E[log D(c_i)] + E[log (1 - D(e_j))] + + 0.1 * E[log (1 - D(G(p_k))]$"
      ]
    }
  ]
}