{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnimeGANPlayground.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFWxWw0ShAcX"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBxrGliTq10J"
      },
      "source": [
        "# import torchvision.models as models\n",
        "# vgg19 = models.vgg19(pretrained=True)\n",
        "# print(vgg19)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcN8al1rrE09"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  \"\"\" Convolution Block made of convolution, instance norm, \n",
        "      and lrelu activation\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel=3, stride=1):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    padding = kernel // 2\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel, stride=stride, padding=padding)\n",
        "    self.inst_norm = nn.InstanceNorm2d(out_channels)\n",
        "    self.lrelu = nn.LeakyReLU(0.2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.inst_norm(x)\n",
        "    x = self.lrelu(x)\n",
        "    return x\n",
        "\n",
        "# cb = ConvBlock(3, 6)\n",
        "# noise_images = torch.randn((64, 3, 256, 256))\n",
        "# out = cb(noise_images)\n",
        "# print(out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjQowj01JTae"
      },
      "source": [
        "class DepthwiseSeparableConv(nn.Module):\n",
        "  \"\"\" depthwise separable convolution layer according to the following posts  \n",
        "  Honestly not too sure if I implemented the depth-wise convolution correctly\n",
        "  or not. Let's cross our fingers and hope I did.\n",
        "  https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec\n",
        "  https://discuss.pytorch.org/t/how-to-modify-a-conv2d-to-depthwise-separable-convolution/15843\n",
        "  \"\"\"  \n",
        "\n",
        "  def __init__(self, in_channels, out_channels, multiplier=4, kernel=3, stride=2):\n",
        "    \"\"\" @spec.requires: in_channels % groups == 0\n",
        "        *this is accounted for by code: in_channels*multiplier. No need be careful\n",
        "    \"\"\"\n",
        "    super(DepthwiseSeparableConv, self).__init__()\n",
        "    self.depthwise = nn.Conv2d(in_channels, in_channels*multiplier, kernel_size=kernel, stride=stride, padding=1, groups=in_channels)\n",
        "    self.pointwise = nn.Conv2d(in_channels*multiplier, out_channels, kernel_size=1, stride=1)    \n",
        "    self.inst_norm = nn.InstanceNorm2d(out_channels)\n",
        "    self.lrelu = nn.LeakyReLU(0.2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\" @param x: [N x C x H x W]\n",
        "        @returns: [N x C x H/stride x W/stride] tensor \n",
        "        Note: Assuming stride is either 1 or 2\n",
        "    \"\"\"\n",
        "    x = self.depthwise(x)\n",
        "    x = self.pointwise(x)\n",
        "    x = self.inst_norm(x)\n",
        "    x = self.lrelu(x)\n",
        "    return x\n",
        "\n",
        "# dwsc = DepthwiseSeparableConv(128,128,kernel=3).cuda()\n",
        "# noise_inputs = torch.randn((64,128,256,256)).cuda()\n",
        "# out = dwsc(noise_inputs)\n",
        "# print(out.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5AwO-v-t3md"
      },
      "source": [
        "class DSConv(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel=3, stride=2):\n",
        "    super(DSConv, self).__init__()\n",
        "    self.depthwise = DepthwiseSeparableConv(in_channels, out_channels,stride=stride)\n",
        "    self.conv_block1 = ConvBlock(out_channels, out_channels, kernel=1, stride=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\" @param x: [N x C x H x W]\n",
        "        @returns: [N x C x H/stride x W/stride] tensor \n",
        "        Note: Assuming stride is either 1 or 2\n",
        "    \"\"\"\n",
        "    x = self.depthwise(x)\n",
        "    # x is [N x out_channels x H/2 x W/2] if stride=2\n",
        "    x = self.conv_block1(x)\n",
        "    return x\n",
        "\n",
        "# ds = DSConv(128,128).cuda()\n",
        "# noise_inputs = torch.randn((64,128,256,256)).cuda()\n",
        "# out = ds(noise_inputs)\n",
        "# print(out.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJt8oP3pGaDk"
      },
      "source": [
        "class InverseResidualBlock(nn.Module):\n",
        "  \"\"\" an inverse residual block :D Let's hope it's right...\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_channels, middle_channels=512):\n",
        "    super(InverseResidualBlock, self).__init__()\n",
        "    self.conv_block = ConvBlock(in_channels, middle_channels, kernel=1,stride=1)\n",
        "    self.dconv = DepthwiseSeparableConv(middle_channels, middle_channels//2, kernel=3, stride=1)\n",
        "    self.conv = nn.Conv2d(middle_channels//2, in_channels, kernel_size=1, stride=1)\n",
        "    self.inst_norm = nn.InstanceNorm2d(in_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\" @param x: [N x C x H x W]\n",
        "        @returns: [N x C x H x W] tensor\n",
        "    \"\"\"\n",
        "    residual = x\n",
        "    x = self.conv_block(x)\n",
        "    x = self.dconv(x)\n",
        "    x = self.conv(x)\n",
        "    x = self.inst_norm(x)\n",
        "    return x + residual\n",
        "\n",
        "# irb = InverseResidualBlock(64,128).cuda()\n",
        "# noise_inputs = torch.randn((64,64,64,64)).cuda()\n",
        "# out = irb(noise_inputs)\n",
        "# print(out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-X6gTdTMahV"
      },
      "source": [
        "class DownConv(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(DownConv, self).__init__()\n",
        "    self.dconv1 = DSConv(in_channels, out_channels, kernel=3, stride=2)\n",
        "    self.dconv2 = DSConv(in_channels, out_channels, kernel=3, stride=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\" @param x: [N x C x H x W]\n",
        "        @returns x: [N x C x H/2 x W/2]\n",
        "    \"\"\"\n",
        "    residual = x\n",
        "    residual = self.dconv1(x)\n",
        "    # residual: [N x out_channels x H/2 x W/2]\n",
        "\n",
        "    x = F.interpolate(x, scale_factor=0.5)\n",
        "    x = self.dconv2(x)\n",
        "    # x: [N x out_channels x H/2 x W/2]\n",
        "    \n",
        "    return x + residual\n",
        "\n",
        "# dc = DownConv(128, 128)\n",
        "# inputs = torch.randn((64,128,128,128))\n",
        "# out = dc(inputs)\n",
        "# print(out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu1v_grbVz6G"
      },
      "source": [
        "class UpConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(UpConv, self).__init__()\n",
        "    self.dconv1 = DSConv(in_channels, out_channels, kernel=3,stride=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\" @param x: [N x C x H x W]\n",
        "        @returns x: [N x C x 2*H x 2*W]\n",
        "    \"\"\"\n",
        "    N, C, H, W = x.shape\n",
        "    x = F.interpolate(x, scale_factor=2)\n",
        "    x = self.dconv1(x)\n",
        "    return x\n",
        "\n",
        "# uc = UpConv(128,128).cuda()\n",
        "# noise_inputs = torch.randn((64,128,4,4)).cuda()\n",
        "# out = uc(noise_inputs)\n",
        "# print(out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTSzSMpRWoAn"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, in_channels=3):\n",
        "    super(Generator, self).__init__()\n",
        "    # encoder stuff\n",
        "    self.conv1 = ConvBlock(3,64)\n",
        "    self.conv2 = ConvBlock(64,64)\n",
        "    self.down_conv1 = DownConv(64,128)\n",
        "    self.conv3 = ConvBlock(128,128)\n",
        "    self.dsconv1 = DepthwiseSeparableConv(128,128,stride=1)\n",
        "    self.down_conv2 = DownConv(128,256)\n",
        "    self.conv4 = ConvBlock(256,256)\n",
        "\n",
        "    # residual layers... Do we even need 8??\n",
        "    # I'll use four for now...\n",
        "    # irb: inverted residual block \n",
        "    self.irb1 = InverseResidualBlock(256)\n",
        "    self.irb2 = InverseResidualBlock(256)\n",
        "    self.irb3 = InverseResidualBlock(256)\n",
        "    self.irb4 = InverseResidualBlock(256)\n",
        "    \n",
        "    # decoder stuff\n",
        "    self.conv5 = ConvBlock(256,256)\n",
        "    self.up_conv1 = UpConv(256, 128)\n",
        "    self.dsconv2 = DepthwiseSeparableConv(128,128,stride=1)\n",
        "    self.conv6 = ConvBlock(128,128)\n",
        "    self.up_conv2 = UpConv(128, 64)\n",
        "    self.conv7 = ConvBlock(64,64)\n",
        "    self.conv8 = ConvBlock(64,64)\n",
        "    self.final_conv_layer = nn.Conv2d(64,3,kernel_size=3,stride=1, padding=1)\n",
        "  \n",
        "  def encode(self, x):\n",
        "    \"\"\" @param x: x is [N x C x H x W] images\n",
        "        @returns: I think its [N x 256 x H/4 x W/4]?\n",
        "    \"\"\"\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.down_conv1(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.dsconv1(x)\n",
        "    x = self.down_conv2(x)\n",
        "    x = self.conv4(x)\n",
        "    return x\n",
        "\n",
        "  def decode(self, x):\n",
        "    \"\"\" @param x: x is [N x C x H x W] image\n",
        "        @returns: I think its [N x 3 x 4*H x 4*W]?\n",
        "    \"\"\"\n",
        "    x = self.conv5(x)\n",
        "    x = self.up_conv1(x)\n",
        "    x = self.dsconv2(x)\n",
        "    x = self.conv6(x)\n",
        "    x = self.up_conv2(x)\n",
        "    x = self.conv7(x)\n",
        "    x = self.conv8(x)\n",
        "    x = self.final_conv_layer(x)\n",
        "    return x\n",
        "    \n",
        "  def residual_forward(self, x):\n",
        "    \"\"\" a forward pass through the residual layers\n",
        "        @param x: [N, 256, H, W] tensor\n",
        "        @returns: [N, 256, H, W] tensor\n",
        "    \"\"\"\n",
        "    x = self.irb1(x)\n",
        "    x = self.irb2(x)\n",
        "    x = self.irb3(x)\n",
        "    x = self.irb4(x)\n",
        "    return x\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\" @param x: [N x C x H x W] images\n",
        "        @returns: [N x C x H x W] images\n",
        "    \"\"\"\n",
        "    x = self.encode(x)\n",
        "    x = self.residual_forward(x)\n",
        "    x = self.decode(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DN6b-9kH_QT",
        "outputId": "7b4d904e-2790-41a4-d15d-fdc8bff3bb31"
      },
      "source": [
        "g = Generator().cuda()\n",
        "noise_images = torch.randn((4,3,256,256)).cuda()\n",
        "recon_images = g(noise_images)\n",
        "print(recon_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3, 256, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP0rvzqBIiko",
        "outputId": "5619d902-d912-4c4a-b7bc-81991b193805"
      },
      "source": [
        "diff = noise_images - recon_images\n",
        "print(diff.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-44527.0508, device='cuda:0', grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-u20wYTK1xX"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(g.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "for i in range(1000):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  recon_images = g(noise_images)\n",
        "  loss = criterion(recon_images, noise_images)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print(loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRldlZ3jNXT2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}