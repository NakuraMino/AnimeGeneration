{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnimeGANTrainer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8e7f4f85ea3a434c8b8df8317a66d1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_75fd09dae02c478eb39c31eccedfc157",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_624e7581e75946ec92281c9583ec1ed4",
              "IPY_MODEL_dac769e2ad7846cb93d7de0d9b649659"
            ]
          }
        },
        "75fd09dae02c478eb39c31eccedfc157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "624e7581e75946ec92281c9583ec1ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14f46460e30246d791d4c8cf53478b09",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0075098bba194472b0a1e6186a186df3"
          }
        },
        "dac769e2ad7846cb93d7de0d9b649659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7bfcaa07fd7b4f2a9b749564ba893f62",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [28:32&lt;00:00, 336kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_075002e195624a62a126e1002a588ff5"
          }
        },
        "14f46460e30246d791d4c8cf53478b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0075098bba194472b0a1e6186a186df3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7bfcaa07fd7b4f2a9b749564ba893f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "075002e195624a62a126e1002a588ff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3NAQjoqkfVT"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.optim as optim \n",
        "from dataloader import *\n",
        "from _generator import *\n",
        "from discriminator import *\n",
        "from losses import * \n",
        "import utilities\n",
        "import random\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BASE_PRETRAIN_PATH = '/content/drive/MyDrive/trained_models/'\n",
        "BASE_SAVE_PATH = '/content/drive/MyDrive/adjustParams/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCqaLv0unM0R",
        "outputId": "3f8be74a-9334-4d95-be7d-efdd86d7f198"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRyG0fWH6ie7"
      },
      "source": [
        "## Initial Variables etc..\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oAQyELFkl59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "8e7f4f85ea3a434c8b8df8317a66d1df",
            "75fd09dae02c478eb39c31eccedfc157",
            "624e7581e75946ec92281c9583ec1ed4",
            "dac769e2ad7846cb93d7de0d9b649659",
            "14f46460e30246d791d4c8cf53478b09",
            "0075098bba194472b0a1e6186a186df3",
            "7bfcaa07fd7b4f2a9b749564ba893f62",
            "075002e195624a62a126e1002a588ff5"
          ]
        },
        "outputId": "0e0be25e-d08a-4d87-a5ed-f0559c2e19fc"
      },
      "source": [
        "# networks \n",
        "\n",
        "generator = Generator().to(device)\n",
        "generator.load_model(BASE_SAVE_PATH + 'generator_checkpoint_e5.pth')\n",
        "generator = generator.train()\n",
        "\n",
        "discriminator = Discriminator().to(device)\n",
        "discriminator.load_model(BASE_SAVE_PATH + 'discriminator_checkpoint_e5.pth')\n",
        "discriminator = discriminator.train()\n",
        "\n",
        "VGG = getVGGConv4_4().to(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e7f4f85ea3a434c8b8df8317a66d1df",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22e8Zisa6lSr"
      },
      "source": [
        "# losses \n",
        "\n",
        "content_loss = ContentLoss(VGG).to(device)\n",
        "grayscale_loss = GrayscaleStyleLoss(VGG).to(device)\n",
        "color_recon_loss = ColorReconLoss().to(device)\n",
        "adversarial_loss = nn.MSELoss().to(device)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD1AnKEc7PT9"
      },
      "source": [
        "# optimizers\n",
        "\n",
        "# maybe come back and add weight decay \n",
        "\n",
        "pre_train_optim = optim.Adam(generator.parameters(), lr=0.0001)\n",
        "\n",
        "gen_optim = optim.Adam(generator.parameters(), lr=0.00008)\n",
        "dis_optim = optim.Adam(discriminator.parameters(), lr=0.00016)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXNqZGJk8unl"
      },
      "source": [
        "ANIME_PATH = '/content/drive/MyDrive/dataset/Shinkai/style/'\n",
        "SMOOTH_PATH = '/content/drive/MyDrive/dataset/Shinkai/smooth/'\n",
        "PHOTOS_PATH = '/content/drive/MyDrive/dataset/train_photo/'\n",
        "\n",
        "photo_dataloader = getPhotoDataloader(PHOTOS_PATH)\n",
        "anime_dataloader = getAnimeDataloader(ANIME_PATH, grayscale=True)\n",
        "dis_dataloader = getPhotoAndAnimeDataloader(ANIME_PATH, SMOOTH_PATH, PHOTOS_PATH)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X98EuO6rC7SX"
      },
      "source": [
        "## Train the Network here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzFOjgybfTIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd3e9d7-1e3d-4d53-b715-3b1acc04bd76"
      },
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "EPOCHS = 10\n",
        "START_EPOCH = 6\n",
        "G_TO_D_RATIO = 5\n",
        "LAMBDA_ADV = 100.\n",
        "LAMBDA_CON = 0.01\n",
        "LAMBDA_GRA = 10.\n",
        "LAMBDA_COL = 5.\n",
        "DISCRIMINATOR_FIRST = False\n",
        "RANDOM_SKIP = 0.3 # roughly 1 to 4 ratio\n",
        "# Math is: \n",
        "# discriminator 6,609 iterations / epoch = 26,438 photos / BATCH_SIZE(4)\n",
        "# generator: 8,320 iterations / epoch = 6,656 images * G_TO_D_RATIO / BATCH_SIZE \n",
        "# discriminator * RANDOM_SKIP / generator is roughly 1 to 4\n",
        "\n",
        "\n",
        "anime_iter = iter(anime_dataloader)\n",
        "\n",
        "fake_true_labels = torch.ones((4,1,64,64)).to(device)\n",
        "\n",
        "content_loss_list = utilities.readListFromPickle(BASE_SAVE_PATH + 'content_loss.pkl')\n",
        "gray_loss_list = utilities.readListFromPickle(BASE_SAVE_PATH + 'gray_loss.pkl')\n",
        "color_loss_list = utilities.readListFromPickle(BASE_SAVE_PATH + 'color_loss.pkl')\n",
        "adv_loss_list = utilities.readListFromPickle(BASE_SAVE_PATH + 'adv_loss.pkl')\n",
        "discriminator_loss_list = utilities.readListFromPickle(f'{BASE_SAVE_PATH}dis_loss.pkl')\n",
        "\n",
        "\n",
        "for e in range(START_EPOCH, EPOCHS):  \n",
        "  print(\"training generator\")\n",
        "  for r in range(G_TO_D_RATIO):\n",
        "    if DISCRIMINATOR_FIRST:\n",
        "      DISCRIMINATOR_FIRST = False\n",
        "      break\n",
        "    for p_batch_idx, photo_batch in enumerate(photo_dataloader):\n",
        "      # train the generator\n",
        "      \n",
        "      gen_optim.zero_grad()\n",
        "      \n",
        "      anime_batch = next(anime_iter).to(device)\n",
        "      if anime_batch.shape != photo_batch.shape:\n",
        "        anime_iter = iter(anime_dataloader)\n",
        "        continue\n",
        "\n",
        "      # pass through generator network\n",
        "      photo_batch = photo_batch.to(device)\n",
        "      gen_images = generator(photo_batch)\n",
        "      # gen_image is [4 x 3 x 256 x 256]\n",
        "\n",
        "      # pass through discriminator network\n",
        "      pred_labels = discriminator(gen_images)\n",
        "\n",
        "      # calculate losses\n",
        "      con_loss = content_loss(gen_images, photo_batch)\n",
        "      gra_loss = grayscale_loss(gen_images, anime_batch)\n",
        "      col_loss = color_recon_loss(gen_images, photo_batch)\n",
        "      adv_loss = adversarial_loss(pred_labels, fake_true_labels)\n",
        "\n",
        "      loss = LAMBDA_ADV * adv_loss + LAMBDA_CON * con_loss + \\\n",
        "              LAMBDA_GRA * gra_loss + LAMBDA_COL * col_loss\n",
        "      \n",
        "      # backpropogate\n",
        "      loss.backward()\n",
        "      gen_optim.step()\n",
        "\n",
        "      # save in list\n",
        "      content_loss_list.append(con_loss.item())\n",
        "      gray_loss_list.append(gra_loss.item())\n",
        "      color_loss_list.append(col_loss.item())\n",
        "      adv_loss_list.append(adv_loss.item())\n",
        "\n",
        "      if p_batch_idx % 500 == 499:\n",
        "        # save model at periodic checkpoints\n",
        "        print(\"generator epoch:\", e, \"r:\", r, \"p_batch_idx\", p_batch_idx, \"loss:\", loss.item())        \n",
        "        utilities.saveListToPickle(BASE_SAVE_PATH + 'content_loss.pkl', content_loss_list)\n",
        "        utilities.saveListToPickle(BASE_SAVE_PATH + 'gray_loss.pkl', gray_loss_list)\n",
        "        utilities.saveListToPickle(BASE_SAVE_PATH + 'color_loss.pkl', color_loss_list)\n",
        "        utilities.saveListToPickle(BASE_SAVE_PATH + 'adv_loss.pkl', adv_loss_list)\n",
        "      \n",
        "    print(\"saving sample generated images...\")\n",
        "    unique_identifier = f\"e{e}r{r}idx\"\n",
        "    utilities.save_torch_as_images(BASE_SAVE_PATH, gen_images, unique_identifier=f'{unique_identifier}', is_standardized_image=True)\n",
        "    utilities.save_torch_as_images(BASE_SAVE_PATH, gen_images, unique_identifier=f'_{unique_identifier}', is_standardized_image=True, adjust_brightness=True, imgs=photo_batch)\n",
        "    print(\"done!\")\n",
        "    generator.save_model(f\"{BASE_SAVE_PATH}generator_checkpoint_e{e}_r{r}.pth\")\n",
        "\n",
        "  generator.save_model(f\"{BASE_SAVE_PATH}generator_checkpoint_e{e}.pth\")\n",
        "  \n",
        "  print(\"training discriminator\")\n",
        "  for batch_idx, (photo_batch, labels) in enumerate(dis_dataloader):\n",
        "    # trains the discriminator\n",
        "\n",
        "    if random.random() < RANDOM_SKIP:\n",
        "      # discriminator is trained too often, so randomly skip RANDOM_SKIP of images\n",
        "      # to prevent discriminator from converging too quickly\n",
        "      continue\n",
        "\n",
        "    dis_optim.zero_grad()\n",
        "    \n",
        "    # send data to cuda if available    \n",
        "    photo_batch = photo_batch.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # pass through discriminator and get loss\n",
        "    pred_labels = discriminator(photo_batch)\n",
        "    loss = LAMBDA_ADV * adversarial_loss(pred_labels, labels)\n",
        "\n",
        "    # backpropogate\n",
        "    loss.backward()\n",
        "    dis_optim.step()\n",
        "    \n",
        "    discriminator_loss_list.append(loss.item())\n",
        "\n",
        "    if batch_idx % 500 == 499:\n",
        "      # save model at periodic checkpoints\n",
        "      print(\"discriminator epoch:\", e, \"batch_idx\", batch_idx, \"loss:\", loss.item())\n",
        "      utilities.saveListToPickle(f'{BASE_SAVE_PATH}dis_loss.pkl', discriminator_loss_list)\n",
        "\n",
        "  discriminator.save_model(f\"{BASE_SAVE_PATH}discriminator_checkpoint_e{e}.pth\")\n",
        "\n",
        "generator.save_model(f\"{BASE_SAVE_PATH}generator_final.pth\")\n",
        "discriminator.save_model(f\"{BASE_SAVE_PATH}discriminator_final.pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training generator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "generator epoch: 6 r: 0 p_batch_idx 499 loss: 1.3788108825683594\n",
            "generator epoch: 6 r: 0 p_batch_idx 999 loss: 1.024960994720459\n",
            "generator epoch: 6 r: 0 p_batch_idx 1499 loss: 0.7302474975585938\n",
            "saving sample generated images...\n",
            "done!\n",
            "generator epoch: 6 r: 1 p_batch_idx 499 loss: 1.0336525440216064\n",
            "generator epoch: 6 r: 1 p_batch_idx 999 loss: 0.9195556640625\n",
            "generator epoch: 6 r: 1 p_batch_idx 1499 loss: 0.7550180554389954\n",
            "saving sample generated images...\n",
            "done!\n",
            "generator epoch: 6 r: 2 p_batch_idx 499 loss: 0.6827806234359741\n",
            "generator epoch: 6 r: 2 p_batch_idx 999 loss: 0.744933009147644\n",
            "generator epoch: 6 r: 2 p_batch_idx 1499 loss: 0.6778417825698853\n",
            "saving sample generated images...\n",
            "done!\n",
            "generator epoch: 6 r: 3 p_batch_idx 499 loss: 0.9143848419189453\n",
            "generator epoch: 6 r: 3 p_batch_idx 999 loss: 0.7374331951141357\n",
            "generator epoch: 6 r: 3 p_batch_idx 1499 loss: 0.5849680304527283\n",
            "saving sample generated images...\n",
            "done!\n",
            "generator epoch: 6 r: 4 p_batch_idx 499 loss: 0.8655440211296082\n",
            "generator epoch: 6 r: 4 p_batch_idx 999 loss: 0.9244300127029419\n",
            "generator epoch: 6 r: 4 p_batch_idx 1499 loss: 0.67930006980896\n",
            "saving sample generated images...\n",
            "done!\n",
            "training discriminator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([4, 1, 1, 1])) that is different to the input size (torch.Size([4, 1, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "discriminator epoch: 6 batch_idx 499 loss: 0.3091875910758972\n",
            "discriminator epoch: 6 batch_idx 999 loss: 0.2818688750267029\n",
            "discriminator epoch: 6 batch_idx 1499 loss: 0.21203672885894775\n",
            "discriminator epoch: 6 batch_idx 1999 loss: 0.06371842324733734\n",
            "discriminator epoch: 6 batch_idx 2499 loss: 0.05788814648985863\n",
            "discriminator epoch: 6 batch_idx 3499 loss: 0.10336916893720627\n",
            "discriminator epoch: 6 batch_idx 3999 loss: 0.07945617288351059\n",
            "discriminator epoch: 6 batch_idx 4999 loss: 0.06535618007183075\n",
            "discriminator epoch: 6 batch_idx 5499 loss: 0.08059097826480865\n",
            "discriminator epoch: 6 batch_idx 6499 loss: 0.1356891542673111\n",
            "training generator\n",
            "generator epoch: 7 r: 0 p_batch_idx 499 loss: 0.8095471858978271\n",
            "generator epoch: 7 r: 0 p_batch_idx 999 loss: 0.6764535903930664\n",
            "generator epoch: 7 r: 0 p_batch_idx 1499 loss: 0.8352744579315186\n",
            "saving sample generated images...\n",
            "done!\n",
            "generator epoch: 7 r: 1 p_batch_idx 499 loss: 0.750571608543396\n",
            "generator epoch: 7 r: 1 p_batch_idx 999 loss: 0.7741347551345825\n",
            "generator epoch: 7 r: 1 p_batch_idx 1499 loss: 0.6727094650268555\n",
            "saving sample generated images...\n",
            "done!\n",
            "generator epoch: 7 r: 2 p_batch_idx 499 loss: 0.6707093119621277\n",
            "generator epoch: 7 r: 2 p_batch_idx 999 loss: 0.6021391153335571\n",
            "generator epoch: 7 r: 2 p_batch_idx 1499 loss: 0.7608623504638672\n",
            "saving sample generated images...\n",
            "done!\n",
            "generator epoch: 7 r: 3 p_batch_idx 499 loss: 0.8899475336074829\n",
            "generator epoch: 7 r: 3 p_batch_idx 999 loss: 0.8117303848266602\n",
            "generator epoch: 7 r: 3 p_batch_idx 1499 loss: 0.751628577709198\n",
            "saving sample generated images...\n",
            "done!\n",
            "generator epoch: 7 r: 4 p_batch_idx 499 loss: 0.5411189198493958\n",
            "generator epoch: 7 r: 4 p_batch_idx 999 loss: 0.6969568729400635\n",
            "generator epoch: 7 r: 4 p_batch_idx 1499 loss: 0.6970996856689453\n",
            "saving sample generated images...\n",
            "done!\n",
            "training discriminator\n",
            "discriminator epoch: 7 batch_idx 499 loss: 0.4782838225364685\n",
            "discriminator epoch: 7 batch_idx 1499 loss: 0.16925334930419922\n",
            "discriminator epoch: 7 batch_idx 1999 loss: 0.07043202966451645\n",
            "discriminator epoch: 7 batch_idx 2499 loss: 0.28684496879577637\n",
            "discriminator epoch: 7 batch_idx 3499 loss: 0.21407410502433777\n",
            "discriminator epoch: 7 batch_idx 3999 loss: 0.17256692051887512\n",
            "discriminator epoch: 7 batch_idx 4499 loss: 0.150628462433815\n",
            "discriminator epoch: 7 batch_idx 4999 loss: 0.8769091367721558\n",
            "discriminator epoch: 7 batch_idx 5999 loss: 0.5477367639541626\n",
            "training generator\n",
            "generator epoch: 8 r: 0 p_batch_idx 499 loss: 0.6180024147033691\n",
            "generator epoch: 8 r: 0 p_batch_idx 999 loss: 0.7710769772529602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS5DCJdvvVKh"
      },
      "source": [
        "# Some quick visual checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JThMDliDr_Sd"
      },
      "source": [
        "pred_labels = discriminator(gen_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67H368eNuuV0"
      },
      "source": [
        "print(pred_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcv1y9K1uvdO"
      },
      "source": [
        "discriminator(photo_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu56K9Xou5tR"
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZeMTGTou7Qe"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# learning curves \r\n",
        "\r\n",
        "INTERVAL = 25\r\n",
        "\r\n",
        "content_loss_list = utilities.listToAvg(utilities.readListFromPickle(BASE_SAVE_PATH + 'content_loss.pkl'), interval=INTERVAL)\r\n",
        "color_loss_list = utilities.listToAvg(utilities.readListFromPickle(BASE_SAVE_PATH + 'color_loss.pkl'), interval=INTERVAL)\r\n",
        "gray_loss_list = utilities.listToAvg(utilities.readListFromPickle(BASE_SAVE_PATH + 'gray_loss.pkl'), interval=INTERVAL)\r\n",
        "adv_loss_list = utilities.listToAvg(utilities.readListFromPickle(BASE_SAVE_PATH + 'adv_loss.pkl'), interval=INTERVAL)\r\n",
        "\r\n",
        "\r\n",
        "f1 = plt.figure()\r\n",
        "plt.title(\"content\")\r\n",
        "plt.plot(content_loss_list)\r\n",
        "\r\n",
        "f2 = plt.figure()\r\n",
        "plt.title(\"color\")\r\n",
        "plt.plot(color_loss_list)\r\n",
        "\r\n",
        "f3 = plt.figure()\r\n",
        "plt.title(\"gray\")\r\n",
        "plt.plot(gray_loss_list)\r\n",
        "\r\n",
        "f4 = plt.figure()\r\n",
        "plt.title(\"adv\")\r\n",
        "plt.plot(adv_loss_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWuP2RZTvczt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}